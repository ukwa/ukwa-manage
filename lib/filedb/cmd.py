
'''
This contains the CLI tool for managing a file DB...
'''
import os
import csv
import sys
import json
import sqlite3
import logging
import argparse

logging.basicConfig(level=logging.WARNING, format='%(asctime)s: %(levelname)s - %(name)s - %(message)s')

logger = logging.getLogger(__name__)



def main():
    # Set up a parser:
    parser = argparse.ArgumentParser(prog='filedb')

    # Common arguments:
    parser.add_argument('-v', '--verbose',  action='count', default=0, help='Logging level; add more -v for more logging.')
    parser.add_argument('--dry-run', action='store_true', help='Do not modify the TrackDB.')
    parser.add_argument('-i', '--indent', type=int, help='Number of spaces to indent when emitting JSON.')

    # Use sub-parsers for different operations:
    subparsers = parser.add_subparsers(dest="op")
    subparsers.required = True

    # 'jsonl-to-sqlite' subcommand - read a file listing generated by hadoop fs -lsr ... and convert to SQLite:
    parser_sq = subparsers.add_parser('jsonl-to-sqlite', help='Read a TrackDB JSONL format file listing and convert to SQLite')
    parser_sq.add_argument('input_jsonl', type=str, help='The file to read, in TrackDB JSONL. Can be "-" for STDIN.')
    parser_sq.add_argument('output_sqlite', type=str, help='The file to output to in SQLite format.')

    # And PARSE it:
    args = parser.parse_args()

    # Set up verbose logging:
    if args.verbose == 1:
        logging.getLogger().setLevel(logging.INFO)    
    elif args.verbose > 1:
        logging.getLogger().setLevel(logging.DEBUG)    

    # Ops:
    logger.debug("Got args: %s" % args)
    if args.op == 'jsonl-to-sqlite':
        # Input
        if args.input_jsonl == '-':
            reader = sys.stdin
        else:
            reader = open(args.input_jsonl, 'r')
        # Output
        con = sqlite3.connect(args.output_sqlite)
        cur = con.cursor()
        cur.execute('''CREATE TABLE IF NOT EXISTS files
               (id TEXT PRIMARY KEY ASC, collection TEXT, stream TEXT, kind TEXT, timestamp DATETIME, store TEXT, file_size INTEGER)''')

        # Convert and write out:
        counter = 0
        for line in reader:
            item = json.loads(line)
            counter += 1
            cur.execute("INSERT OR IGNORE INTO files(id, collection, stream, kind, timestamp, store, file_size) \
                VALUES( '%(id)s', '%(collection_s)s', '%(stream_s)s', '%(kind_s)s',  '%(timestamp_dt)s',  '%(hdfs_service_id_s)s', %(file_size_l)s )" % item)

        # Close up
        if reader is not sys.stdin:
            reader.close()
        con.commit()
        con.close()

        # Check this seems to have worked:
        if counter == 0:
            raise Exception("No records were found for conversion!")

if __name__ == "__main__":
    main()
