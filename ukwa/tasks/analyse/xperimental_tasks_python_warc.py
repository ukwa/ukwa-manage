import os
import io
import sys
import logging
import luigi
import luigi.contrib.hdfs
import luigi.contrib.hadoop
import warcio
import six
from six.moves.urllib.parse import urlparse

logger = logging.getLogger('luigi-interface')

#
# This is a Python-based streaming Hadoop job for performing basic processing of warcs
# and e.g. generating stats. However all implementations (`warctools`, `warc` and `pywb`) require
# behaviour that is 'difficult' to support. For the first two, both use Python's gzip support,
# which requires seekable streams (e.g. `seek(offset,whence)` support). Python Wayback (`pywb`)
# does not appear to depend on that module, but required `ffi` support for native calls, which
# makes deployment more difficult. Therefore, we use Java map-reduce jobs for WARC parsing, but
# we can generate simple line-oriented text files from the WARCs, after which streaming works
# just fine.
#
# Further experimentation with warcio shows this seems to be working better as it has very few dependencies.
#
# Also attempted to use Hadoop's built-in auto-gunzipping support, which is built into streaming mode.
# After some difficulties, this could be made to work, but was unreliable as different nodes would behave differently
# with respect to keeping-going when gunzipping concateneted gz records.
#


class ExternalListFile(luigi.ExternalTask):
    """
    This ExternalTask defines the Target at the top of the task chain. i.e. resources that are overall inputs rather
    than generated by the tasks themselves.
    """
    input_file = luigi.Parameter()

    def output(self):
        """
        Returns the target output for this task.
        In this case, it expects a file to be present in HDFS.
        :return: the target output for this task.
        :rtype: object (:py:class:`luigi.target.Target`)
        """
        return luigi.contrib.hdfs.HdfsTarget(self.input_file)


class GenerateWarcStatsIndirect(luigi.contrib.hadoop.JobTask):
    """
    Generates the WARC stats by reading each file in turn. Data is therefore no-local.

    Parameters:
        input_file: The file (on HDFS) that contains the list of WARC files to process
    """
    input_file = luigi.Parameter()

    def output(self):
        out_name = "%s-stats.tsv" % os.path.splitext(self.input_file)[0]
        return luigi.contrib.hdfs.HdfsTarget(out_name, format=luigi.contrib.hdfs.PlainDir)

    def requires(self):
        return ExternalListFile(self.input_file)

    def extra_modules(self):
        return []

    def extra_files(self):
        return ["luigi.cfg"]

    def mapper(self, line):
        """
        Each line should be a path to a WARC file on HDFS

        We open each one in turn, and scan the contents.

        The pywb record parser gives access to the following properties (at least):

        entry['urlkey']
        entry['timestamp']
        entry['url']
        entry['mime']
        entry['status']
        entry['digest']
        entry['length']
        entry['offset']

        :param line:
        :return:
        """

        # Ignore blank lines:
        if line == '':
            return

        warc = luigi.contrib.hdfs.HdfsTarget(line)
        #entry_iter = DefaultRecordParser(sort=False,
        #                                 surt_ordered=True,
       ##                                  include_all=False,
        #                                 verify_http=False,
        #                                 cdx09=False,
        #                                 cdxj=False,
        #                                 minimal=False)(warc.open('rb'))

        #for entry in entry_iter:
        #    hostname = urlparse.urlparse(entry['url']).hostname
        #    yield hostname, entry['status']

    def reducer(self, key, values):
        """

        :param key:
        :param values:
        :return:
        """
        for value in values:
            yield key, sum(values)


class ExternalFilesFromList(luigi.ExternalTask):
    """
    This ExternalTask defines the Target at the top of the task chain. i.e. resources that are overall inputs rather
    than generated by the tasks themselves.
    """
    input_file = luigi.Parameter()
    from_local = luigi.BoolParameter(default=False)

    def output(self):
        """
        Returns the target output for this task.
        In this case, it expects a file to be present in HDFS.
        :return: the target output for this task.
        :rtype: object (:py:class:`luigi.target.Target`)
        """
        for line in open(self.input_file, 'r').readlines():
            line = line.strip()
            if line:
                if self.from_local:
                    yield luigi.LocalTarget(path=line)
                else:
                    yield luigi.contrib.hdfs.HdfsTarget(line, format=luigi.contrib.hdfs.format.PlainFormat)


# Special reader to read the input stream and yield WARC records:
class TellingReader():
    def __init__(self, stream):
        self.stream = io.open(stream.fileno(), 'rb')
        self.pos = 0

    def read(self, size=None):
        logger.warning("read()ing from current position: %i" % self.pos)
        chunk = self.stream.read(size)
        logger.warning("read() %s" % chunk)
        self.pos += len(chunk)
        logger.warning("read()ing current position now: %i" % self.pos)
        return chunk

    def readline(self, size=None):
        logger.warning("readline()ing from current position: %i" % self.pos)
        line = self.stream.readline(size)
        logger.warning("readline() %s" % line)
        self.pos += len(bytes(line))
        logger.warning("readline()ing current position now: %i" % self.pos)
        return line

    def tell(self):
        logger.warning("tell()ing current position: %i" % self.pos)
        return self.pos


class BinaryInputHadoopJobRunner(luigi.contrib.hadoop.HadoopJobRunner):
    """
    A job runner to use the UnsplittableInputFileFormat (based on DefaultHadoopJobRunner):
    """

    def __init__(self):
        config = luigi.configuration.get_config()
        streaming_jar = config.get('hadoop', 'streaming-jar')
        super(BinaryInputHadoopJobRunner, self).__init__(
            streaming_jar=streaming_jar,
            input_format="uk.bl.wa.hadoop.mapred.UnsplittableInputFileFormat",
            libjars=["../jars/warc-hadoop-recordreaders-2.2.0-BETA-7-SNAPSHOT-job.jar"])


class HadoopWarcReaderJob(luigi.contrib.hadoop.JobTask):
    """
    Specialisation of the usual Hadoop JobTask that is configured to parse warc files.

    Should be sub-classed to make tasks that work with WARCs

    As this uses the stream directly and so data-locality is preserved (at least for the first chunk).

    Parameters:
        input_file: The path for the file that contains the list of WARC files to process
        from_local: Whether the paths refer to files on HDFS or local files
        read_for_offset: Whether the WARC parser should read the whole record so it can populate the
                         record.raw_offset and record.raw_length fields (good for CDX indexing). Enabling this will
                         mean the reader has consumed the content body so your job will not have access to it.
    """
    input_file = luigi.Parameter()
    from_local = luigi.BoolParameter(default=False)
    read_for_offset = luigi.BoolParameter(default=False)

    def requires(self):
        return ExternalFilesFromList(self.input_file, from_local=self.from_local)

    def extra_files(self):
        return ["luigi.cfg"]

    def extra_modules(self):
        return [warcio, six]

    def job_runner(self):
        outputs = luigi.task.flatten(self.output())
        for output in outputs:
            if not isinstance(output, luigi.contrib.hdfs.HdfsTarget):
                logger.warn("Job is using one or more non-HdfsTarget outputs" +
                              " so it will be run in local mode")
                return luigi.contrib.hadoop.LocalJobRunner()
        else:
            return BinaryInputHadoopJobRunner()

    def run_mapper(self, stdin=sys.stdin, stdout=sys.stdout):
        """
        Run the mapper on the hadoop node.

        ANJ: Creating modified version to pass through the raw stdin
        """
        self.init_hadoop()
        self.init_mapper()
        outputs = self._map_input(stdin)
        if self.reducer == NotImplemented:
            self.writer(outputs, stdout)
        else:
            self.internal_writer(outputs, stdout)

    def _map_input(self, input_stream):
        """
        Iterate over input and call the mapper for each item.
        If the job has a parser defined, the return values from the parser will
        be passed as arguments to the mapper.

        If the input is coded output from a previous run,
        the arguments will be splitted in key and value.

        ANJ: Modified to use the warcio parser instead of splitting lines.
        """
        reader = warcio.ArchiveIterator(input_stream)
        for record in reader:
            if self.read_for_offset:
                record.raw_offset = reader.get_record_offset()
                record.raw_length = reader.get_record_length()
            for output in self.mapper(record):
                yield output
        if self.final_mapper != NotImplemented:
            for output in self.final_mapper():
                yield output
        self._flush_batch_incr_counter()


class GenerateWarcStats(HadoopWarcReaderJob):
    """
    Generates the Warc stats by reading in each file and splitting the stream into entries.

    See :py:class:`HadoopWarcReaderJob` for details of the job parameters.
    """

    def output(self):
        out_name = "%s-stats.tsv" % os.path.splitext(self.input_file)[0]
        if self.from_local:
            return luigi.LocalTarget(out_name)
        else:
            return luigi.contrib.hdfs.HdfsTarget(out_name, format=luigi.contrib.hdfs.PlainFormat)

    def mapper(self, record):
        """

        Simple mapper takes the parsed WARC record and extracts some basic stats.

        :param record:
        :return:
        """

        if (record.rec_type == 'response' and record.content_type.startswith(b'application/http')):

            # Extract
            record_url = record.rec_headers.get_header('WARC-Target-URI')
            status_code = record.http_headers.get_statuscode()

            if self.read_for_offset:
                print(record.raw_offset, record.raw_length, record.length, record.content_stream().read())
            else:
                print(record.length, record.content_stream().read())

            hostname = urlparse(record_url).hostname
            yield "%s\t%s" % (hostname, status_code), 1

    def reducer(self, key, values):
        """

        :param key:
        :param values:
        :return:
        """
        # for value in values:
        yield key, sum(values)


if __name__ == '__main__':
    luigi.run(['GenerateWarcStats', '--input-file', '/Users/andy/Documents/workspace/python-shepherd/input-files.txt', '--from-local', '--local-scheduler'])
